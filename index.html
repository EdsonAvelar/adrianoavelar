<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Precisamos falar de dados | DataBlow</title>

    <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Precisamos falar de dados | DataBlow</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Precisamos falar de dados" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Um blog desenvolvido por @adrianoavelar." />
<meta property="og:description" content="Um blog desenvolvido por @adrianoavelar." />
<link rel="canonical" href="http://www.adrianoavelar.com/" />
<meta property="og:url" content="http://www.adrianoavelar.com/" />
<meta property="og:site_name" content="DataBlow" />
<script type="application/ld+json">
{"@type":"WebSite","url":"http://www.adrianoavelar.com/","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://www.adrianoavelar.com/assets/images/logo.png"}},"name":"DataBlow","headline":"Precisamos falar de dados","description":"Um blog desenvolvido por @adrianoavelar.","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <link rel="shortcut icon" type="image/x-icon" href="/assets/images/favicon.ico">

    <!-- Font Awesome Icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">

    <!-- Google Fonts-->
    <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

    <!-- Bootstrap Modified -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!-- Theme Stylesheet -->
    <link rel="stylesheet" href="/assets/css/theme.css">

    <!-- Jquery on header to make sure everything works, the rest  of the scripts in footer for fast loading -->
    <script
    src="https://code.jquery.com/jquery-3.3.1.min.js"
    integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8="
    crossorigin="anonymous"></script>

    <!-- This goes before </head> closing tag, Google Analytics can be placed here --> 


</head>

<body class=" homefirstpage ">

    <!-- Navbar -->
    <nav id="MagicMenu" class="topnav navbar navbar-expand-lg navbar-light bg-white fixed-top">
    <div class="container">
        <a class="navbar-brand" href="/index.html"><strong>DataBlow</strong></a>
        <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
        </button>
        <div class="navbar-collapse collapse" id="navbarColor02" style="">
            <ul class="navbar-nav mr-auto d-flex align-items-center">
               <!--  Replace menu links here -->

<li class="nav-item">
<a class="nav-link" href="/index.html">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/authors-list.html">Authors</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/contact.html">Contact</a>
</li>
<li class="nav-item">
    <a class="nav-link" href="/portfolio-pt.html">Portfolio</a>
</li>
            </ul>
            <ul class="navbar-nav ml-auto d-flex align-items-center">
                <script src="/assets/js/lunr.js"></script>

<script>
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 1000 );
        $( "body" ).removeClass( "modal-open" );
    });
});
    

var documents = [{
    "id": 0,
    "url": "http://www.adrianoavelar.com/404/",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "http://www.adrianoavelar.com/about.html",
    "title": "About",
    "body": "Feito com by Adriano Avelar "
    }, {
    "id": 2,
    "url": "http://www.adrianoavelar.com/author-adriano.html",
    "title": "adriano",
    "body": "                        adriano Follow:         /portfolio-pt. html         Olá, me chamo Adriano Avelar, criador do blog Datablow, seja bem vindo                                   Posts by adriano:                   		Aprendizado por Reforço - Introdução 	: 		  Introdução a Aprendizagem por Reforço	 			In 				reinforcement_learning, 								Jul 24, 2019						            		Curso de Spark e Python: Aula 04 - Operações Básicas	: 		  Curso de Spark e PythonAula 04 - Operações Bàsicas	 			In 				curso_spark, 								Jun 26, 2019						            		Curso de Spark e Python: Aula 03 - Hello World	: 		  Curso de Spark e PythonAula 03 - DataFrame II	 			In 				curso_spark, 								Jun 26, 2019						            		Curso de Spark e Python: Aula 02 - DataFrame I	: 		  Curso de Spark e PythonAula 02 - DataFrame I Para essa aula, você já deverá ter um ambiente spark instalado e configurado. 	 			In 				curso_spark, 								Jun 23, 2019						            		Curso de Spark e Python: Aula 01 - Introdução	: 		  Curso de Spark e PythonAula 01 - IntroduçãoOlá, seja bem vindo ao Curso de Spark e Python do Adriano Avelar. Caso encontre algum erro ou deseja entrar em contato, mande um e-mail para. . . 	 			In 				curso_spark, 								Jun 23, 2019						            		Primeiro Post	: 		  Olá. Me chamo Adriano Avelar, sou Engenheiro da Computação formado pela Universidade Federal do Pará. Tenho Doutorado em Computação pela Universidade Federal de Pernambuco. Já morei e. . . 	 			In 				portfolio, 								Jan 18, 2019						        "
    }, {
    "id": 3,
    "url": "http://www.adrianoavelar.com/authors-list.html",
    "title": "Authors",
    "body": "Authors:                                             Adriano :       (View Posts)      Olá, me chamo Adriano Avelar, criador do blog Datablow, seja bem vindo                           &nbsp;       &nbsp;                                      "
    }, {
    "id": 4,
    "url": "http://www.adrianoavelar.com/buy-me-a-coffee.html",
    "title": "Buy me a coffee",
    "body": "Hi! I am Sal, web designer &amp; developer at WowThemes. net. The free items I create are my side projects and Mundana for Jekyll is one of them. You can find all the work I release for free here. You have my permission to use the free items I develop in your personal, commercial or client projects. If you’d like to reward my work, I would be honored and I could dedicate more time maintaining the free projects. Thank you so much! Buy me a coffee "
    }, {
    "id": 5,
    "url": "http://www.adrianoavelar.com/categories.html",
    "title": "Categories",
    "body": "          Categories               portfolio:                                  		Primeiro Post	: 		  Olá. Me chamo Adriano Avelar, sou Engenheiro da Computação formado pela Universidade Federal do Pará. Tenho Doutorado em Computação pela Universidade Federal de Pernambuco. Já morei e. . . 	 			In 				portfolio, 								Jan 18, 2019						                              curso_spark:                                  		Curso de Spark e Python: Aula 04 - Operações Básicas	: 		  Curso de Spark e PythonAula 04 - Operações Bàsicas	 			In 				curso_spark, 								Jun 26, 2019						                                 		Curso de Spark e Python: Aula 03 - Hello World	: 		  Curso de Spark e PythonAula 03 - DataFrame II	 			In 				curso_spark, 								Jun 26, 2019						                                 		Curso de Spark e Python: Aula 02 - DataFrame I	: 		  Curso de Spark e PythonAula 02 - DataFrame I Para essa aula, você já deverá ter um ambiente spark instalado e configurado. 	 			In 				curso_spark, 								Jun 23, 2019						                                 		Curso de Spark e Python: Aula 01 - Introdução	: 		  Curso de Spark e PythonAula 01 - IntroduçãoOlá, seja bem vindo ao Curso de Spark e Python do Adriano Avelar. Caso encontre algum erro ou deseja entrar em contato, mande um e-mail para. . . 	 			In 				curso_spark, 								Jun 23, 2019						                              reinforcement_learning:                                  		Aprendizado por Reforço - Introdução 	: 		  Introdução a Aprendizagem por Reforço	 			In 				reinforcement_learning, 								Jul 24, 2019						                                             Featured:    				                                          Aprendizado por Reforço - Introdução                           Em                     reinforcement_learning,                                                                                           Curso de Spark e Python: Aula 04 - Operações Básicas                          Em                     curso_spark,                                                                                           Curso de Spark e Python: Aula 03 - Hello World                          Em                     curso_spark,                                                                                           Curso de Spark e Python: Aula 02 - DataFrame I                          Em                     curso_spark,                                                                                           Curso de Spark e Python: Aula 01 - Introdução                          Em                     curso_spark,                                                                                           Primeiro Post                          Em                     portfolio,                                                                   "
    }, {
    "id": 6,
    "url": "http://www.adrianoavelar.com/contact.html",
    "title": "Contact",
    "body": "  Please send your message to DataBlow. We will reply as soon as possible!   "
    }, {
    "id": 7,
    "url": "http://www.adrianoavelar.com/",
    "title": "Precisamos falar de dados",
    "body": "                                  Aprendizado por Reforço - Introdução   :       Introdução a Aprendizagem por Reforço               Em                 reinforcement_learning,                                        Jul 24, 2019                                                                                                                                              Curso de Spark e Python: Aula 04 - Operações Básicas          :                       Em                         curso_spark,                                                                  Jun 26, 2019                                                                                                                                             Curso de Spark e Python: Aula 03 - Hello World          :                       Em                         curso_spark,                                                                  Jun 26, 2019                                                                                                                                            Curso de Spark e Python: Aula 02 - DataFrame I          :                       Em                         curso_spark,                                                                  Jun 23, 2019                                                       All Stories:                   		Aprendizado por Reforço - Introdução 	: 		  Introdução a Aprendizagem por Reforço	 			In 				reinforcement_learning, 								Jul 24, 2019						                  		Curso de Spark e Python: Aula 04 - Operações Básicas	: 		  Curso de Spark e PythonAula 04 - Operações Bàsicas	 			In 				curso_spark, 								Jun 26, 2019						                  		Curso de Spark e Python: Aula 03 - Hello World	: 		  Curso de Spark e PythonAula 03 - DataFrame II	 			In 				curso_spark, 								Jun 26, 2019						                  		Curso de Spark e Python: Aula 02 - DataFrame I	: 		  Curso de Spark e PythonAula 02 - DataFrame I Para essa aula, você já deverá ter um ambiente spark instalado e configurado. 	 			In 				curso_spark, 								Jun 23, 2019						                  		Curso de Spark e Python: Aula 01 - Introdução	: 		  Curso de Spark e PythonAula 01 - IntroduçãoOlá, seja bem vindo ao Curso de Spark e Python do Adriano Avelar. Caso encontre algum erro ou deseja entrar em contato, mande um e-mail para. . . 	 			In 				curso_spark, 								Jun 23, 2019						                  		Primeiro Post	: 		  Olá. Me chamo Adriano Avelar, sou Engenheiro da Computação formado pela Universidade Federal do Pará. Tenho Doutorado em Computação pela Universidade Federal de Pernambuco. Já morei e. . . 	 			In 				portfolio, 								Jan 18, 2019						                                                  Featured:    				                                          Aprendizado por Reforço - Introdução                           Em                     reinforcement_learning,                                                                                           Curso de Spark e Python: Aula 04 - Operações Básicas                          Em                     curso_spark,                                                                                           Curso de Spark e Python: Aula 03 - Hello World                          Em                     curso_spark,                                                                                           Curso de Spark e Python: Aula 02 - DataFrame I                          Em                     curso_spark,                                                                                           Curso de Spark e Python: Aula 01 - Introdução                          Em                     curso_spark,                                                                                           Primeiro Post                          Em                     portfolio,                                                               "
    }, {
    "id": 8,
    "url": "http://www.adrianoavelar.com/portfolio-en.html",
    "title": "portfolio Adriano",
    "body": " Versão em Português EDSON ADRIANO MARAVALHO AVELAR RUA TÓKIO 31, ANANINDEUA, PA 67013-050eam. avelar@gmail. comLattes: http://lattes. cnpq. br/5014272753295806 SHORT BIO: Após me formar em Engenharia da Computação, em 2010, em Belém-Pa, fui para Recife fazer mestrado e doutorado, ambos em Ciências da Computação. Lá trabalhei em pesquisas com empresas como Ericsson e RNP. Trabalhei como programador Java em um sistema de Cloud Privada durante 1 ano. Após isso, voltei para Belém e abri uma Empresa de Ensino de Programação. Nesse meio tempo virei professor da Universidade Federal Rural da Amazônia, em disciplinas ligadas à inteligência artificial. Em 2018 conheci a plataforma Kaggle e me apaixonei pela Ciências de Dados. Hoje trabalho sou Cientista de Dados em uma grande empresa do setor financeiro. FORMAÇÃO ACADÊMICA: Bacharelado em Engenharia da Computação: TCC: Predição de Mobilidade Usando Redes NeuraisUniversidade Federal do Pará ( Belém,PA | 2010 ) Mestrado em Ciências da Computação: Dissertação: PMIPFlow: Uma proposta para gerenciamento de mobilidade em redes definidas por software. ( Redes Definidas por Software)Universidade Federal de Pernambuco (Recife,PE | 2013 ) Doutorado em Ciências da Computação: Tese: MobCache: Mobilidade de provedor e armazenamento eficiente de conteúdo em redes ccn sem fioUniversidade Federal de Pernambuco (Recife,PE | 2017 ) Projetos:  EduFlow - Eduroam e Handover em Redes OpenFlow - Rede Nacional de Pesquisa SIMTUR (Sistema Inteligente de Monitoramento de Tráfego Urbano) - Rede Nacional de Pesquisa (2013) ReVir (Redes Virtuais na Internet do Futuro) - Rede Nacional de Pesquisa Dynapol (Dynamic Policy Negotiation in Mobile and Fixed Environments) - Ericsson Telecomunicações Para ver Publicações Cientificas: Consultar Lattes HISTÓRICO PROFISSIONAL: (Empresa do Setor Financeiro): Cientista de Dados | São Paulo | 2020 - Atual Universidade Federal Rural da Amazônia: Professor Universitário | Belém, Pará | Abril 2018 – Maio 2020Fui professor substituo durante 2 períodos, 2014 até 2015 e 2018 até 2020.  Ministrei diversas disciplinas ligadas a Programação, Inteligência Artificial, Arquitetura e Redes. 2014. 2 Auditoria e Segurança de Sistemas - 68h Redes de Computadores - 68h Sistemas Distribuidos - 68h2015. 1 Inteligência Artificial - 68h Introdução a Computação- 68h Redes de Computadores II - 68h2018. 1 Arquitetura de Computadores I - 68h Informática Aplicada - 51h Informática Básica - 34h Informática Básica - 34h Introdução à Computação Gráfica - 68h Introdução à Inteligência Artificial - 34h Tópicos Especiais em Software - 68h2019. 1 Introdução à Inteligência Artificial - 68h Linguagens de Programação - 68h Técnicas de Programação I - 68h2019. 2 Arquitetura de Computadores II - 68h Auditoria e Segurança de Sistemas - 68hSuperGeeks Belém (Franquia): Desenvolvedor, Coordenador Pedagógico e Sócio | Belém, Pará | Jan 2016 – Maio 2020Coordenador Pedagógico, Professor e Desenvolvedor do Sistema Pedagógico da Empresa. Sistema feito em PHP no Framework Laravel. Professor do Curso Ciências de Dados na Prática: Professor | Belém, Pará | Maio/Jun 2019Também ministrei cursos de introdução a Ciências de Dados. https://www. sympla. com. br/curso-ciencias-de-dados-belem---modulo-1__514819 USTORE: Desenvolvedor Java | Recife, PE | Jan 2013 – Jan 2014Programador do Sistema USTORE de armazenamento em nuvem privada. Trabalhava com as camadas internas do sistema, comunicação e criptografia. Me desliguei da empresa porque precisei entrar em um projeto de pesquisa no Doutorado. COMPETÊNCIAS: Conhecimento em linguagens de Programação:  PHP ( Intermediário ) Linguagem de desenvolvimento do sistema pedagógico da SuperGeeks Belém.  obs: Faz tempo que não trabalho.  C/C++ (Avançado) Usei na criação de simuladores para Mestrado/Doutorado obs: Faz tempo que não trabalho.  Java (Avançado) Linguagem que usei quando trabalhava na empresa UStore (Recife) obs: Faz tempo que não trabalho Python (Avançado) Aplicado em projetos de Ciências de Dados e Inteligência Artificial.  Atualmente é a linguagem que tenho mais contato Scala ( Básico ) Estou estudando Scala atualmente devido ao SparkCompetições do Kaggle: Fiz várias competições no Kaggle. Em minha opinião, é umas das melhores fontes de conhecimento em análise de dados. Estou no nível Expert Notebook na plataforma, meu objetivo é chegar até GrandMaster (nível mais alto). Acesse o link abaixo para ver meu perfil:https://www. kaggle. com/adrianoavelar Abaixo vou destacar as competições mais relevantes que fiz. 1 Predicting Molecular Properties:  Tipo: Regressão Objetivo: Prever uma constante escalar chamada scalar_coupling_constant a partir dos dados fornecidos.  Porque ela é importante: Precisei estudar muita quimica para entender o problema e criar novas soluções. Foi uma competição bem complexa.    Principais Ganhos:Tive 4 Notebooks importantes:   Medalha de Ouro. 191 Votos: https://www. kaggle. com/adrianoavelar/eachtype   Medalha de Ouro. 145 Votos: https://www. kaggle. com/adrianoavelar/bond-calculaltion-lb-0-82   Medalha de Ouro. 118 Votos: https://www. kaggle. com/adrianoavelar/bond-calculation-lb-0-82   Medalha de Prata: 28 Votos: https://www. kaggle. com/adrianoavelar/molecule-pred-feature-eng-lb-0-685  2 University of Liverpool - Ion Switching:  Tipo: Classificação Objetivo: Prever o número de canais de íons abertos em uma célula Porque ela é importante: Precisei estudar bastante biologia para entender o problema e criar novas soluções. Foi uma das competições mais desafiadoras e interessantes que participei.    Principais Ganhos:Tive 1 Notebook importante:   Medalha de Ouro. 90 Votos: https://www. kaggle. com/adrianoavelar/3-simple-ideas-lb-0-938  OBSERVAÇÕES FINAIS: Espero que você tenha gostado do meu portfolio. Qualquer duvidas ou sugetões, entre em contato comigo pelo email: eam. avelar@gmail. com. Abraços Adriano Avelar "
    }, {
    "id": 9,
    "url": "http://www.adrianoavelar.com/portfolio-pt.html",
    "title": "Portfolio Adriano",
    "body": " English Version EDSON ADRIANO MARAVALHO AVELAR RUA TÓKIO 31, ANANINDEUA, PA 67013-050eam. avelar@gmail. comLattes: http://lattes. cnpq. br/5014272753295806 SHORT BIO: Após me formar em Engenharia da Computação, em 2010, em Belém-Pa, fui para Recife fazer mestrado e doutorado, ambos em Ciências da Computação. Lá trabalhei em pesquisas com empresas como Ericsson e RNP. Trabalhei como programador Java em um sistema de Cloud Privada durante 1 ano. Após isso, voltei para Belém e abri uma Empresa de Ensino de Programação. Nesse meio tempo virei professor da Universidade Federal Rural da Amazônia, em disciplinas ligadas à inteligência artificial. Em 2018 conheci a plataforma Kaggle e me apaixonei pela Ciências de Dados. Hoje trabalho sou Cientista de Dados em uma grande empresa do setor financeiro. FORMAÇÃO ACADÊMICA: Bacharelado em Engenharia da Computação: Universidade Federal do Pará ( Belém,PA | 2010 )TCC: Predição de Mobilidade Usando Redes Neurais Mestrado em Ciências da Computação: Universidade Federal de Pernambuco (Recife,PE | 2013 ) Dissertação: PMIPFlow: Uma proposta para gerenciamento de mobilidade em redes definidas por software. Doutorado em Ciências da Computação: Universidade Federal de Pernambuco (Recife,PE | 2017 ) Tese: MobCache: Mobilidade de provedor e armazenamento eficiente de conteúdo em redes ccn sem fio Projetos:  EduFlow - Eduroam e Handover em Redes OpenFlow - Rede Nacional de Pesquisa SIMTUR (Sistema Inteligente de Monitoramento de Tráfego Urbano) - Rede Nacional de Pesquisa (2013) ReVir (Redes Virtuais na Internet do Futuro) - Rede Nacional de Pesquisa Dynapol (Dynamic Policy Negotiation in Mobile and Fixed Environments) - Ericsson Telecomunicações Para ver Publicações Cientificas: Consultar Lattes HISTÓRICO PROFISSIONAL: (Empresa do Setor Financeiro): Cientista de Dados | São Paulo | 2020 - Atual Universidade Federal Rural da Amazônia: Professor Universitário | Belém, Pará | Abril 2018 – Maio 2020Fui professor substituo durante 2 períodos, 2014 até 2015 e 2018 até 2020.  Ministrei diversas disciplinas ligadas a Programação, Inteligência Artificial, Arquitetura e Redes. 2014. 2 Auditoria e Segurança de Sistemas - 68h Redes de Computadores - 68h Sistemas Distribuidos - 68h2015. 1 Inteligência Artificial - 68h Introdução a Computação- 68h Redes de Computadores II - 68h2018. 1 Arquitetura de Computadores I - 68h Informática Aplicada - 51h Informática Básica - 34h Informática Básica - 34h Introdução à Computação Gráfica - 68h Introdução à Inteligência Artificial - 34h Tópicos Especiais em Software - 68h2019. 1 Introdução à Inteligência Artificial - 68h Linguagens de Programação - 68h Técnicas de Programação I - 68h2019. 2 Arquitetura de Computadores II - 68h Auditoria e Segurança de Sistemas - 68hSuperGeeks Belém (Franquia): Desenvolvedor, Coordenador Pedagógico e Sócio | Belém, Pará | Jan 2016 – Maio 2020Coordenador Pedagógico, Professor e Desenvolvedor do Sistema Pedagógico da Empresa. Sistema feito em PHP no Framework Laravel. Professor do Curso Ciências de Dados na Prática: Professor | Belém, Pará | Maio/Jun 2019Também ministrei cursos de introdução a Ciências de Dados. https://www. sympla. com. br/curso-ciencias-de-dados-belem---modulo-1__514819 USTORE: Desenvolvedor Java | Recife, PE | Jan 2013 – Jan 2014Programador do Sistema USTORE de armazenamento em nuvem privada. Trabalhava com as camadas internas do sistema, comunicação e criptografia. Me desliguei da empresa porque precisei entrar em um projeto de pesquisa no Doutorado. COMPETÊNCIAS: Conhecimento em linguagens de Programação:  PHP ( Intermediário ) Linguagem de desenvolvimento do sistema pedagógico da SuperGeeks Belém.  obs: Faz tempo que não trabalho.  C/C++ (Avançado) Usei na criação de simuladores para Mestrado/Doutorado obs: Faz tempo que não trabalho.  Java (Avançado) Linguagem que usei quando trabalhava na empresa UStore (Recife) obs: Faz tempo que não trabalho Python (Avançado) Aplicado em projetos de Ciências de Dados e Inteligência Artificial.  Atualmente é a linguagem que tenho mais contato Scala ( Básico ) Estou estudando Scala atualmente devido ao SparkCompetições do Kaggle: Fiz várias competições no Kaggle. Em minha opinião, é umas das melhores fontes de conhecimento em análise de dados. Estou no nível Expert Notebook na plataforma, meu objetivo é chegar até GrandMaster (nível mais alto). Acesse o link abaixo para ver meu perfil:https://www. kaggle. com/adrianoavelar Abaixo vou destacar as competições mais relevantes que fiz. 1 Predicting Molecular Properties:  Tipo: Regressão Objetivo: Prever uma constante escalar chamada scalar_coupling_constant a partir dos dados fornecidos.  Porque ela é importante: Precisei estudar muita quimica para entender o problema e criar novas soluções. Foi uma competição bem complexa.    Principais Ganhos:Tive 4 Notebooks importantes:   Medalha de Ouro. 191 Votos: https://www. kaggle. com/adrianoavelar/eachtype   Medalha de Ouro. 145 Votos: https://www. kaggle. com/adrianoavelar/bond-calculaltion-lb-0-82   Medalha de Ouro. 118 Votos: https://www. kaggle. com/adrianoavelar/bond-calculation-lb-0-82   Medalha de Prata: 28 Votos: https://www. kaggle. com/adrianoavelar/molecule-pred-feature-eng-lb-0-685  2 University of Liverpool - Ion Switching:  Tipo: Classificação Objetivo: Prever o número de canais de íons abertos em uma célula Porque ela é importante: Precisei estudar bastante biologia para entender o problema e criar novas soluções. Foi uma das competições mais desafiadoras e interessantes que participei.    Principais Ganhos:Tive 1 Notebook importante:   Medalha de Ouro. 90 Votos: https://www. kaggle. com/adrianoavelar/3-simple-ideas-lb-0-938  OBSERVAÇÕES FINAIS: Espero que você tenha gostado do meu portfolio. Qualquer duvidas ou sugetões, entre em contato comigo pelo email: eam. avelar@gmail. com. Abraços Adriano Avelar "
    }, {
    "id": 10,
    "url": "http://www.adrianoavelar.com/privacy-policy.html",
    "title": "Privacy Policy",
    "body": "”{{site. name}}” takes your privacy seriously. To better protect your privacy we provide this privacy policy notice explaining the way your personal information is collected and used. Collection of Routine Information: This website track basic information about their visitors. This information includes, but is not limited to, IP addresses, browser details, timestamps and referring pages. None of this information can personally identify specific visitor to this website. The information is tracked for routine administration and maintenance purposes. Cookies: Where necessary, this website uses cookies to store information about a visitor’s preferences and history in order to better serve the visitor and/or present the visitor with customized content. Advertisement and Other Third Parties: Advertising partners and other third parties may use cookies, scripts and/or web beacons to track visitor activities on this website in order to display advertisements and other useful information. Such tracking is done directly by the third parties through their own servers and is subject to their own privacy policies. This website has no access or control over these cookies, scripts and/or web beacons that may be used by third parties. Learn how to opt out of Google’s cookie usage. Links to Third Party Websites: We have included links on this website for your use and reference. We are not responsible for the privacy policies on these websites. You should be aware that the privacy policies of these websites may differ from our own. Security: The security of your personal information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your personal information, we cannot guarantee its absolute security. Changes To This Privacy Policy: This Privacy Policy is effective and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page. We reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. If we make any material changes to this Privacy Policy, we will notify you either through the email address you have provided us, or by placing a prominent notice on our website. Contact Information: For any questions or concerns regarding the privacy policy, please contact us here. "
    }, {
    "id": 11,
    "url": "http://www.adrianoavelar.com/tags.html",
    "title": "Tags",
    "body": "          Tags          {% for tag in site. tags %}     {{ tag[0] }}:           {% assign pages_list = tag[1] %}    {% for post in pages_list %}    {% if post. title != null %}     {% if group == null or group == post. group %}           {% include main-loop-card. html %}     {% endif %}    {% endif %}    {% endfor %}    {% assign pages_list = nil %}    {% assign group = nil %}    {% endfor %}                  {% include sidebar-featured. html %}          "
    }, {
    "id": 12,
    "url": "http://www.adrianoavelar.com/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 13,
    "url": "http://www.adrianoavelar.com/rf/introducao.html",
    "title": "Aprendizado por Reforço - Introdução ",
    "body": "2019/07/24 - Introdução a Aprendizagem por ReforçoMuitos dos conceitos aqui presentes foram retirados do livro:Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow - Aurélien Géron Aprendizado por Reforço (Reinformecent Learning) é um dos campos mais excitantes do Aprendizado de Máquina e também um dos mais antigos. Hoje existem diversas aplicações, principalmente em jogos e controles de máquinas (robôs, carros autônomos). Na industria o RL ainda é pouco explorado, por dois motivos: 1) Maturidade: Hoje a preocupação maior é em obter, armazenar e tratar os dados. Existe o hype na industria de que as decisões orientadas a dados é importante, porém grande parte das empresas ainda não tem infraestrutura orientadas a dados. 2) Relevância: Tem um pouco a ver com a maturidade. As empresas ainda não acham relevante aplicar Aprendizado por Reforço em seus produtos. Em muitas delas a área de Ciência de Dados quase não tem Inteligência, é mais um conjunto de regras e filtros do que inteligẽncia em si. Porém, com a maturidade cada vez maior e o aumento da relevãncia da área de dados, novas possibilidades estão surgindo no mercado. Isso permite que a Ciẽncias (Experimentação) se torne atraente também dentro das empresas. Poder do Aprendizado por Reforço: Sem dúvida, o aprendizado por reforço é uma das área mais promissoras. O que separa um simples modelo de uma tarefa complexa é apenas o tempo de treinamento. Em 2014, uma empresa chamada DeepMind foi comprada pelo google por 500 milhões de dolares. Ela cria tecnologias baseadas em RL para todos os tipos de problemas. Na época ela ficou famosa por demonstrar que um sistema simples era capaz de aprender a jogar um jogo do Atari. Em meados de 2016, uma de suas criações AlphaGo, ganhou o campeão mundial do jogo GO. Hoje o google utiliza muitas das tecnologias da DeepMind para melhorar seus próprios produtos. Principalmente utilizando sistemas de recomendação, detecção de SPAM e ofertas personalizadas para cada perfil de usuário.  Aprendizado por ~Reforço~ Recompensa: No RL, um Agente oberva um Ambiente e executa determinadas Ações que vão lhe trazer maior Recompensa. Cada posição do Agente é chamada de estado.  O objetivo do Agente é aprender um jeito de realizar uma tarefa que dê o maior soma total de recompensas Pense na imagem abaixo, o objetivo do Mario é chegar até a princesa. O Mário não sabe o percurso até a princesa, então ele precisa achar o caminho por meio de tentativa e erro. Se ele fizer algo de errado, como por exemplo ir para uma casa onde tem um inimigo ele receberá uma recompensa negativa. Caso ele chegue na princesa, ganha uma recompensa positiva No final, após várias tentativas, o nosso querido mário terá em maos um MAPA com o melhor caminho partindo de qualquer lugar (qualquer estado).  Policy Search: O termo Policy é dificil de traduzir. Poderia ser políticas, conjunto de regras ou estratégia. O Fato é que a Policy é algoritmo usado pelo agente para tomar as decisões. Em nosso exemplo do Mário, a Policy seria o Mapa. Inicialmente o Mapa vem em branco e é preenchido com os estados e as ações do mario durante sua trajetória. Policy Search ou (Busca pela Política) é a busca pelas ações que o agente tem que tomar dado que ele está em um determinado estado. Chama-se política ótima para a política que fornece a maior soma total de recompensas. Existem várias formas de buscar uma política ótima ( ou preencher o melhor Mapa do Mario) 1) Força Bruta: Poderiamos testar para cada posição do mário qual a melhor ação. Em um cenário pequeno isso é simples de fazer. Mas pense em um robô aspirador de pó, o número de estados é muito grande ou até infinito. 2) Algoritmo Genético: Para ambientes onde o conjutno de estados é muito grande poderia-se utilziar algoritmos genético.  a) Cria-se aleatoriamente algumas politicas b) Testa as politicas e elimina as piores c) Cria-se novas politicas a apartir da combinação das melhores politicas d) Repete esse processo até achar uma política satisfatória. É um boa estratégia, mas precisa de poder computacional e o tempo de execução por ser um grande problema. 3) Policy Gradient: É uma técnica de otimização bastante utilizada. O gradiente utiliza derivadas para achar valores mínimos (Gradiente Descendente) ou valores máximos (Gradiente Ascendente). Quem já conhece as redes neurais já ouviu fazer do Gradiente Descendente. No algoritmo de backprograpação, o gradiente é usado para atualizar os pesos da rede neural através da minimização do erro total. No caso da RL, o objetivo é maximizar a recompensa, por isso, é utilizado o Gradiente Ascendente.  Ambiente (Environment): Um dos principais desafios do Reinforcement Learning é o ambiente. Pense em um agente aprendendo a jogar Dota2, famoso jogo de estratégia entre equipes (gif abaixo). O ambiente são todas as arvores, as torres, os inimigos tudo.  OpenAI Vence Campeões Mundiais de Dota 2 Agora, imagine que estamos programando um robozinho virtual para aprender a caminhar usando 2 pernas, como na figura abaixo. O ambiente são as descidas, subidas, obstáculos etc.  E se você tiver fazendo um sistema de aprendizado por reforço para dar recomendações de filmes, como acontecer no NetFlix por exemplo? O agente seria o modelo e o ambiente seria o conjunto de caracteristicas dos filmes mais informações dos usuário por exemplo. O objetivo do modelo seria, para um conjunto de caracteristas de um usuário e dos filmes do catálogo, qual a melhor recomendação para este usuário. Pŕoximo Post Vamos para o HandsOn: "
    }, {
    "id": 14,
    "url": "http://www.adrianoavelar.com/spark/aula04_basic_operations.html",
    "title": "Curso de Spark e Python: Aula 04 - Operações Básicas",
    "body": "2019/06/26 - Curso de Spark e PythonAula 04 - Operações Bàsicas: import findsparkfindspark. init()from pyspark. sql import SparkSessionspark = SparkSession. builder. appName( Aula04 ). getOrCreate()Vamos baixar o dataset desta aula. !wget 'http://adrianoavelar. com/uploads/datasets/appl_stock. csv' -O stock. csv--2020-06-26 11:31:52-- http://adrianoavelar. com/uploads/datasets/appl_stock. csvResolving adrianoavelar. com (adrianoavelar. com). . . 107. 180. 27. 152Connecting to adrianoavelar. com (adrianoavelar. com)|107. 180. 27. 152|:80. . . connected. HTTP request sent, awaiting response. . . 200 OKLength: 143130 (140K) [text/csv]Saving to: ‘stock. csv’stock. csv       9%[&gt;          ] 13,62K --. -KB/s  eta 4m 42s ^CCarregando os dados em . csv para DataFrame Spark:  Pede para inferir o Schema e informa que o arquivo possui cabeçalho df = spark. read. csv('stock. csv', inferSchema = True, header = True )df. printSchema()root |-- Date: timestamp (nullable = true) |-- Open: double (nullable = true) |-- High: double (nullable = true) |-- Low: double (nullable = true) |-- Close: double (nullable = true) |-- Volume: integer (nullable = true) |-- Adj Close: double (nullable = true)Vamos dar uma olhada na tabela usando o pandas. Lembre-se que o pandas não é a forma correta de manipular dados distribuidos em Big Data. Mas podemos utilizá-lo para algumas visualizações pequenas. Recomendo utilizar o limit para limitar a tabela antes de usar o pandas. df. limit(10). toPandas()         Date   Open   High   Low   Close   Volume   Adj Close         0   2010-01-04   213. 429998   214. 499996   212. 380001   214. 009998   123432400   27. 727039       1   2010-01-05   214. 599998   215. 589994   213. 249994   214. 379993   150476200   27. 774976       2   2010-01-06   214. 379993   215. 230000   210. 750004   210. 969995   138040000   27. 333178       3   2010-01-07   211. 750000   212. 000006   209. 050005   210. 580000   119282800   27. 282650       4   2010-01-08   210. 299994   212. 000006   209. 060005   211. 980005   111902700   27. 464034       5   2010-01-11   212. 799997   213. 000002   208. 450005   210. 110003   115557400   27. 221758       6   2010-01-12   209. 189995   209. 769995   206. 419998   207. 720001   148614900   26. 912110       7   2010-01-13   207. 870005   210. 929995   204. 099998   210. 650002   151473000   27. 291720       8   2010-01-14   210. 110003   210. 459997   209. 020004   209. 430000   108223500   27. 133657       9   2010-01-15   210. 929995   211. 599997   205. 869999   205. 930000   148516900   26. 680198   Filtrando os dados: O comando filter é usado para filtrar dados. Existem várias formas de filtros. Primeiramente vamos ver o modo SQL de filtro.  Filtrando dados da coluna Close menores que 500df. filter( Close &lt; 500 ). limit(100). toPandas()         Date   Open   High   Low   Close   Volume   Adj Close         0   2010-01-04   213. 429998   214. 499996   212. 380001   214. 009998   123432400   27. 727039       1   2010-01-05   214. 599998   215. 589994   213. 249994   214. 379993   150476200   27. 774976       2   2010-01-06   214. 379993   215. 230000   210. 750004   210. 969995   138040000   27. 333178       3   2010-01-07   211. 750000   212. 000006   209. 050005   210. 580000   119282800   27. 282650       4   2010-01-08   210. 299994   212. 000006   209. 060005   211. 980005   111902700   27. 464034       . . .    . . .    . . .    . . .    . . .    . . .    . . .    . . .        95   2010-05-20   241. 880009   243. 849987   236. 209999   237. 759995   320728800   30. 804078       96   2010-05-21   232. 819988   244. 499989   231. 349995   242. 319992   305972800   31. 394869       97   2010-05-24   247. 279999   250. 900002   246. 260002   246. 759987   188559700   31. 970113       98   2010-05-25   239. 349991   246. 759987   237. 160007   245. 220005   262001600   31. 770594       99   2010-05-26   250. 080009   252. 129990   243. 750011   244. 109993   212663500   31. 626781   100 rows × 7 columns  Selecione as colunas Open e Close apenas quando os dados de Close forem maiores que 200. df. select(['Open','Close']). filter('Close &gt; 200 '). toPandas()         Open   Close         0   213. 429998   214. 009998       1   214. 599998   214. 379993       2   214. 379993   210. 969995       3   211. 750000   210. 580000       4   210. 299994   211. 980005       . . .    . . .    . . .        144   252. 360008   253. 070011       145   252. 839993   249. 880005       146   249. 390007   249. 639999       147   251. 790009   245. 799992       148   242. 669987   239. 930000   149 rows × 2 columns Ao invés de usar a notação SQL. Podemos utilizar o modo Python de FiltroNesse caso utilizaremos o objeto pyspark. sql. column. Column type(df['Close'])pyspark. sql. column. ColumnO comando de filtro é: df. filter( df['Close'] &lt; 200). toPandas()         Date   Open   High   Low   Close   Volume   Adj Close         0   2010-01-22   206. 780006   207. 499996   197. 160000   197. 750000   220441900   25. 620401       1   2010-01-28   204. 930004   205. 500004   198. 699995   199. 289995   293375600   25. 819922       2   2010-01-29   201. 079996   202. 199995   190. 250002   192. 060003   311488100   24. 883208       3   2010-02-01   192. 369997   196. 000000   191. 299999   194. 729998   187469100   25. 229131       4   2010-02-02   195. 909998   196. 319994   193. 379993   195. 859997   174585600   25. 375533       5   2010-02-03   195. 169994   200. 200003   194. 420004   199. 229994   153832000   25. 812149       6   2010-02-04   196. 730003   198. 370001   191. 570005   192. 050003   189413000   24. 881912       7   2010-02-05   192. 630003   196. 000000   190. 850002   195. 460001   212576700   25. 323710       8   2010-02-08   195. 690006   197. 880003   193. 999994   194. 119997   119567700   25. 150100       9   2010-02-09   196. 419996   197. 499994   194. 749998   196. 190004   158221700   25. 418289       10   2010-02-10   195. 889997   196. 600000   194. 260000   195. 120007   92590400   25. 279660       11   2010-02-11   194. 880001   199. 750006   194. 059996   198. 669994   137586400   25. 739595       12   2010-02-23   199. 999998   201. 330002   195. 709993   197. 059998   143773700   25. 531005    Seleciona apenas a coluna Volume quando a coluna Low for menor que 192df. filter( df['Low'] &lt; 192). select('Volume'). toPandas()         Volume         0   311488100       1   187469100       2   189413000       3   212576700   As vezes é necessário filtrar usando múltiplas condições. Existem duas formas de fazer isso:  Criando filters em sequência. Nesse caso não temos a ideia do OU condicionaldf. filter( df['Low'] &lt; 192). filter( df['Close'] &lt; 200 ). select( ['Close','Low'] ). toPandas()         Close   Low         0   192. 060003   190. 250002       1   194. 729998   191. 299999       2   192. 050003   191. 570005       3   195. 460001   190. 850002    Concatenando multiplas condições Podemos utilizar diversos tipos de condicionais. Porém, duas observações são importantes:     Os simbolos de condicionais mudam: usar ‘&amp;’ para ‘and’ usar ‘|’ para ‘or’ usar ‘~’ para ‘not’   As expressões precisam ser separadas por parênteses. Errado: df[‘Low’] &lt; 192 &amp; df[‘Close’] &lt; 200 Certo: (df[‘Low’] &lt; 192) &amp; (df[‘Close’] &lt; 200)    Selecione as colunas Close e Low quando Low for menor que 192 e Close menor que 200df. filter( (df['Low'] &lt; 192) &amp; (df['Close'] &lt; 200) ). select( ['Close','Low'] ). toPandas()         Close   Low         0   192. 060003   190. 250002       1   194. 729998   191. 299999       2   192. 050003   191. 570005       3   195. 460001   190. 850002    Selecione os dados quando a coluna Low for 197. 16df. filter(df['Low'] == 197. 16). toPandas()         Date   Open   High   Low   Close   Volume   Adj Close         0   2010-01-22   206. 780006   207. 499996   197. 16   197. 75   220441900   25. 620401   Vamos supor que você precise guardar essa informação de Linha em uma variável. Geralmente você deseja trabalhar com o objecto Row. Usa-se a função collect para isso.  Collect (Ação) - Retorna todos os elementos do conjunto de dados como uma matriz. Isso geralmente é útil após um filtro ou outra operação que retorna um subconjunto suficientemente pequeno dos dados. row = df. filter(df['Low'] == 197. 16). collect()row[Row(Date=datetime. datetime(2010, 1, 22, 0, 0), Open=206. 78000600000001, High=207. 499996, Low=197. 16, Close=197. 75, Volume=220441900, Adj Close=25. 620401)]Pode-se pegar os elementos diretamente com indices de list row[0][1]206. 78000600000001Pode-se pegas os resultados como um dicionário row[0]. asDict(){'Date': datetime. datetime(2010, 1, 22, 0, 0), 'Open': 206. 78000600000001, 'High': 207. 499996, 'Low': 197. 16, 'Close': 197. 75, 'Volume': 220441900, 'Adj Close': 25. 620401}row[0]. asDict()['High']207. 499996"
    }, {
    "id": 15,
    "url": "http://www.adrianoavelar.com/spark/aula03_dataframe2.html",
    "title": "Curso de Spark e Python: Aula 03 - Hello World",
    "body": "2019/06/26 - Curso de Spark e PythonAula 03 - DataFrame II: Seja bem vindo a mais uma aula sobre Spark e Python. Hoje vamos continuar nossos estudos sobre Dataframe Spark. Vamos fazer o mesmo da aula passada.  Primeiro utilizamos o findspark para pegar o pacote pysparkimport findsparkfindspark. init() Importamos o SparkSession e criamos uma Sessão Sparkfrom pyspark. sql import SparkSessionspark = SparkSession. builder. appName( HelloWorld ). getOrCreate() Criamos nosso Schema para controlar a importação os dadosfrom pyspark. sql. types import (StructField, StringType,               IntegerType, StructType)data_schema = StructType( [ StructField('age',IntegerType(),True),       StructField('name',StringType(),True)] ) Importamos os dados e criamos um DataFramedf = spark. read. json('uploads/data/people. json', schema = data_schema ) Finalmente imprimimos o Schema para saber se está tudo certo. df. printSchema() Selecionando os dados (select): Vamos analisar os dois comandos abaixo. Ambos tentam pegar a coluna ‘age’, porém o primeiro utiliza um sintaxe semelhante ao pandas e o segundo utiliza a função select. No primeiro caso o retorno é um objeto Column. type(df['age'] )pyspark. sql. column. ColumnO problema é que não é possível manipular um objeto Column. Ele não foi feito para isso. Veja o comando abaixo. df['age']. show()---------------------------------------------------------------------------TypeError                 Traceback (most recent call last)&lt;ipython-input-15-176e0fb11baa&gt; in &lt;module&gt;----&gt; 1 df['age']. show()TypeError: 'Column' object is not callableO DataFrame é o objeto ideal para manipulação. Para obter um DataFrame utiliza-se o comando Select. type(df. select('age') )pyspark. sql. dataframe. DataFrameAgora sim é possível obter as informações, pois estamos trabalhando com um DataFrame. df. select('age'). show()+----+| age|+----+|null|| 30|| 19|+----+Para obter as primeiras linhas de um dataframe usa-se o comando head #Pega as duas primeiras linhasdf. head(2)[Row(age=None, name='Michael'), Row(age=30, name='Andy')]Obtendo multiplas colunas df. select(['age','name']). show()+----+-------+| age|  name|+----+-------+|null|Michael|| 30|  Andy|| 19| Justin|+----+-------+Criando e Adicionando Novas Colunas (WithColumn): A criação de novas colunas é uma das operações mais comuns no dia dia de quem trabalha com spark. Essa operação é realizada com a função withColumn df. withColumn('newage', df['age'] ). show()+----+-------+------+| age|  name|newage|+----+-------+------+|null|Michael| null|| 30|  Andy|  30|| 19| Justin|  19|+----+-------+------+É possível fazer transformações na coluna criada df. withColumn('double_age', df['age'] * 2). show()+----+-------+----------+| age|  name|double_age|+----+-------+----------+|null|Michael|   null|| 30|  Andy|    60|| 19| Justin|    38|+----+-------+----------+A nova coluna pode ser uma expressão df. withColumn('jovem', df['age'] &lt; 20). show()+----+-------+------+| age|  name|newage|+----+-------+------+|null|Michael| null|| 30|  Andy| false|| 19| Justin| true|+----+-------+------+Mas lembre-se, um dataframe é imutável. df. show() mostrará o mesmo resultado de antes do withColumn df. show()+----+-------+| age|  name|+----+-------+|null|Michael|| 30|  Andy|| 19| Justin|+----+-------+Não é possível acrescentar uma coluna ao dataframe original, mas podemos criar outro dataframe com a coluna adicionada. df1 = df. withColumn('jovem', df['age'] &lt; 20)df1. show()+----+-------+-----+| age|  name|jovem|+----+-------+-----+|null|Michael| null|| 30|  Andy|false|| 19| Justin| true|+----+-------+-----+Outra operação comum é renomear colunas. df. withColumnRenamed('age','age2'). show()+----+-------+|age2|  name|+----+-------+|null|Michael|| 30|  Andy|| 19| Justin|+----+-------+Usando SQL Puro: Com Spark Dataframe é possivel realizar operações como se tiveremos trabalhando com banco SQL.  Primeiramente, vamos transformar nosso dataframe em um TempView. df. createOrReplaceTempView('people')Vamos criar e executar a Query SQL, guardamos o resultado em results sql_query =  SELECT * FROM people results = spark. sql(sql_query )Mostramos os resultados e voilá! results. show()+----+-------+| age|  name|+----+-------+|null|Michael|| 30|  Andy|| 19| Justin|+----+-------+Outro exemplo: sql_query =  SELECT * FROM people WHERE age=30 results = spark. sql(sql_query )results. show()+---+----+|age|name|+---+----+| 30|Andy|+---+----+Em nosso curso o foco não será em operações SQL e sim em Python. Ficaremos por aqui. Aula 04 "
    }, {
    "id": 16,
    "url": "http://www.adrianoavelar.com/spark/aula02_dataframe1.html",
    "title": "Curso de Spark e Python: Aula 02 - DataFrame I",
    "body": "2019/06/23 - Curso de Spark e PythonAula 02 - DataFrame I:  Para essa aula, você já deverá ter um ambiente spark instalado e configurado.  O primeiro passo é iniciar uma sessão Spark. Mas antes, precisamos achar o spark que está instalado na máquina. A biblioteca findspark faz esse trabalho para nós. Caso não tenha ela instalado, você pode digitar no terminal: pip install findspark ou no próprio jupyter: !pip install findsparkRequirement already satisfied: findspark in /home/adriano/anaconda3/lib/python3. 7/site-packages (1. 4. 2)import findsparkfindspark. init()Soment após o findspark. init() que o pacote pyspark estará disponivel para importação (comando a seguir) from pyspark. sql import SparkSessionAgora, vamos criar uma sessão spark. #appName: Nome da aplicação#getOrCreate: Pega uma sessão se ela já existir, caso contrário criar uma nova spark = SparkSession. builder. appName( HelloWorld ). getOrCreate()Agora vamos importar alguns dados de um arquivo e criar um DataFrame Spark. Você pode utilizar o arquivo que desejar, no formato que desejar. Vamos começar um simples Arquivo json: { name : Michael }{ name : Andy ,  age :30}{ name : Justin ,  age :19}df = spark. read. json('uploads/data/people. json')df. show()+----+-------+| age|  name|+----+-------+|null|Michael|| 30|  Andy|| 19| Justin|+----+-------+Perceba que o Spark automaticamente substitui os valores faltosos como nullPara imprimir o Schema do DataFrame utilize a função printSchema df. printSchema()root |-- age: long (nullable = true) |-- name: string (nullable = true)O Spark também já inferiu os dados de acordo com o tipo que está sendo armazenado. Outro comando útil é o columns, que não é uma função e sim um parâmetro do dataframe df. columns['age', 'name']Para saber um resumo básico estatistico dos dados, podemos usar a função describe df. describe()DataFrame[summary: string, age: string, name: string]Aqui vem um detalhe interessante. O Spark usa o princípio Lazy Evaluation. Ou seja, ele não executa as transformações até que uma ação é chamada. Nesse caso, describe é um transformação, portanto, não é executada até que uma ação seja solicitada. Essa estratégia permite acumular transformações e executar todas de uma única vez, o que traz um grande ganho de desempenho. Para ver o resultado do describe() use o comando show(). Como show é uma ação, as transformações que vem antes serão executadas. df. describe(). show()+-------+------------------+-------+|summary|        age|  name|+-------+------------------+-------+| count|         2|   3||  mean|       24. 5|  null|| stddev|7. 7781745930520225|  null||  min|        19|  Andy||  max|        30|Michael|+-------+------------------+-------+Que tal ver os resultados através do Pandas? df. describe(). toPandas()         summary   age   name         0   count   2   3       1   mean   24. 5   None       2   stddev   7. 7781745930520225   None       3   min   19   Andy       4   max   30   Michael   Muito melhor não é? ⛔ Mas cuidado. O pandas só pode ser utilizado quando os resultados são pequenos. Ele não foi feito para grandes massas de dados. Utilize com muita cautela.  Caso você não consiga rodar o comando acima, é possível que você não tenha o pandas em seu computador. O comando a seguir resolve isso. !pip install pandasRequirement already satisfied: pandas in /home/adriano/anaconda3/lib/python3. 7/site-packages (1. 0. 1)Requirement already satisfied: pytz&gt;=2017. 2 in /home/adriano/anaconda3/lib/python3. 7/site-packages (from pandas) (2019. 3)Requirement already satisfied: numpy&gt;=1. 13. 3 in /home/adriano/anaconda3/lib/python3. 7/site-packages (from pandas) (1. 18. 1)Requirement already satisfied: python-dateutil&gt;=2. 6. 1 in /home/adriano/anaconda3/lib/python3. 7/site-packages (from pandas) (2. 8. 1)Requirement already satisfied: six&gt;=1. 5 in /home/adriano/anaconda3/lib/python3. 7/site-packages (from python-dateutil&gt;=2. 6. 1-&gt;pandas) (1. 14. 0)Criando o próprio Schema: Muitas vezes você necessita que seu dados estejam de tipos diferentes dos que o Spark inferiu. Por exemplo, em nosso caso, ele inferiu que a coluna age é do tipo long, se quisermos mudar para int, por exemplo, precisamos criar nosso proprio Schema. Então é isso que vamos fazer. Primeiramente importamos nossos pacotes necessários. from pyspark. sql. types import (StructField, StringType,               IntegerType, StructType)Depois criamos uma lista com StructFields informando 3 parâmetros em cada:  1: Nome da coluna alvo.  2: Tipo de dados.  3: Se o campo pode ser nulo. data_schema = StructType( [ StructField('age',IntegerType(),True),       StructField('name',StringType(),True)] )Agora, vamos carregar novamente os dados mas dessa vez passando o nosso schema criado. df = spark. read. json('uploads/data/people. json', schema = data_schema )df. printSchema()root |-- age: integer (nullable = true) |-- name: string (nullable = true)Link da Próxima Aula:Aula 03 "
    }, {
    "id": 17,
    "url": "http://www.adrianoavelar.com/spark/aula01_introducao.html",
    "title": "Curso de Spark e Python: Aula 01 - Introdução",
    "body": "2019/06/23 - Curso de Spark e PythonAula 01 - Introdução: Olá, seja bem vindo ao Curso de Spark e Python do Adriano Avelar. Caso encontre algum erro ou deseja entrar em contato, mande um e-mail para [eam. avelar@gmail. com] Nesse série de artigos, você irá aprender como usar o Spark com Python, incluindo Spark Streaming, Machine Learningg, Spark 2. 0, Spark Sql e muito mais. Habilidades Recomendadas: Para que esse curso seja proveitoso para você, são necessárias as seguintes habilidades:    Básico de Programação (principalmente python)Se você sabe criar um função, usar um loop while ou for, saber um poucos de listas. Já está valendo. Se não sabe nada dessas coisas sugiro procurar no youtube algum curso básico de Python.     Interesse em Big Data. Analisar grandes dados é diferente de analisar pequenos dados. E quando falo grande, digo na ordem de terabytes ou vários gigabytes. Se o seu interesse não é analisar grandes massas de dados, esse não é o curso ideal.  O que não vamos ver: Como disse, esse curso é voltado para Big Data. Portanto, não iremos trabalhar com bibliotecas comuns do python, como numpy, pandas, scikit-learn etc. Essas bibliotecas não conseguem manipular uma grande massa de dados. Mesmo assim, iremos utilizar o pandas e matplotlib para melhorar a visualização de alguns resultados menores. Todo o curso será feito em jupyter, mas você pode utilizar qualquer outra forma de programar o Spark. Principais Topicos: Veremos uma série de tópicos nesse curso, abaixo descrevo apenas os principais.  Conceitos de Spark Spark Dataframes Spark Sql Introdução à Machine Learning em BigData Regressão Linear Regressão Logística Àrvores de Decisão Random Forest Gradient Boost Machine K-means Clustering Processamento de Linguagem Natural (Natural Language Processing) Spark StreamingTambém veremos um pouco sobre pipeline e validação cruzada. Link da Próxima Aula:Aula 02 "
    }, {
    "id": 18,
    "url": "http://www.adrianoavelar.com/myfirst-post/",
    "title": "Primeiro Post",
    "body": "2019/01/18 - Olá. Me chamo Adriano Avelar, sou Engenheiro da Computação formado pela Universidade Federal do Pará. Tenho Doutorado em Computação pela Universidade Federal de Pernambuco. Já morei em Belém, Recife e agora estou em São Paulo. Já fui Programador, Professor Universitário e já tive uma escola de programação para crianças. Hoje trabalho como Cientista de Dados em um grande empresa. Objetivo do Blog: O objetivo deste blog é passar o conhecimento que adquiri ao longo de minha jornado. Como professor sempre me preocupei muito em compartilhar o conhecimento. Espero que você goste do conteúdo e caso ache algum erro ou queira entrar em contato comigo, me envie um e-mail: eam. avelar@gmail. com "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});


    
function lunr_search(term) {
    $('#lunrsearchresults').show( 1000 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-secondary btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
</script>
<style>
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>




<form class="bd-search hidden-sm-down" onSubmit="return lunr_search(document.getElementById('lunrsearch').value);">
<input type="text" class="form-control text-small"  id="lunrsearch" name="q" value="" placeholder="Type keyword and enter..."> 
</form>
            </ul>
        </div>
    </div>
    </nav>

    <!-- Search Results -->
    <div id="lunrsearchresults">
        <ul class="mb-0"></ul>
    </div>

    <!-- Content -->
    <main role="main" class="site-content">
        <div class="container">
    

    
    
    
<!-- Begin post excerpts, let's highlight the first 4 posts on top -->
<div class="row remove-site-content-margin">
    
    <!-- latest post -->
    
    <div class="col-md-6">
    <div class="card border-0 mb-4 box-shadow">   
    <a href="/rf/introducao.html">
    <div class="topfirstimage" style="background-image: url(https://img.ibxk.com.br/2018/10/15/corpos-15114949494090.gif); height: 200px;    background-size: cover;    background-repeat: no-repeat;"></div>     
    </a>
    <div class="card-body px-0 pb-0 d-flex flex-column align-items-start">
    <h2 class="h4 font-weight-bold">
    <a class="text-dark" href="/rf/introducao.html">Aprendizado por Reforço - Introdução </a>
    </h2>
    <p class="excerpt">
        Introdução a Aprendizagem por Reforço
    </p>
    <div>
        <small class="d-block text-muted">
            Em <span class="catlist">
                
                <a class="text-capitalize text-muted smoothscroll" href="/categories.html#reinforcement_learning">reinforcement_learning</a><span class="sep">, </span>
                
                </span>                   
        </small>
        <small class="text-muted">
            Jul 24, 2019
        </small>
    </div>
    </div>
    </div>
    </div>
    
    <div class="col-md-6">
        
        <!-- second latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/spark/aula04_basic_operations.html">
                 <!-- img class="w-100" src="https://penseemti.com.br/wp-content/uploads/2017/02/A-importa%CC%82ncia-do-Banco-de-Dados.jpg" alt="Curso de Spark e Python: Aula 04 - Operações Básicas" -->
                 
                 <img class="w-100" src="https://penseemti.com.br/wp-content/uploads/2017/02/A-importa%CC%82ncia-do-Banco-de-Dados.jpg" alt="Curso de Spark e Python: Aula 04 - Operações Básicas">

                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/spark/aula04_basic_operations.html">Curso de Spark e Python: Aula 04 - Operações Básicas</a>
                    </h2>
                    <small class="d-block text-muted">
                        Em <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#curso_spark">curso_spark</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Jun 26, 2019
                    </small>
                </div>
            </div>
        
        <!-- third latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/spark/aula03_dataframe2.html">
                 <!-- img class="w-100" src="https://blog.wavy.global/wp-content/uploads/2019/07/shahadat-shemul-BfrQnKBulYQ-unsplash-780x450.jpg" alt="Curso de Spark e Python: Aula 03 - Hello World" -->
                 <img class="w-100" src="https://blog.wavy.global/wp-content/uploads/2019/07/shahadat-shemul-BfrQnKBulYQ-unsplash-780x450.jpg" alt="Curso de Spark e Python: Aula 03 - Hello World">

                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/spark/aula03_dataframe2.html">Curso de Spark e Python: Aula 03 - Hello World</a>
                    </h2>
                    <small class="d-block text-muted">
                        Em  <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#curso_spark">curso_spark</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Jun 26, 2019
                    </small>
                </div>
            </div>
        
        <!-- fourth latest post --><div class="mb-3 d-flex align-items-center">                
                
                <div class="col-md-4">
                <a href="/spark/aula02_dataframe1.html">
                <!-- img class="w-100" src="https://files.keepingcurrentmatters.com/wp-content/uploads/2020/05/05155523/20200506-KCM-Shar.jpg" alt="Curso de Spark e Python: Aula 02 - DataFrame I" -->
                <img class="w-100" src="https://files.keepingcurrentmatters.com/wp-content/uploads/2020/05/05155523/20200506-KCM-Shar.jpg" alt="Curso de Spark e Python: Aula 02 - DataFrame I">
                </a>
                </div>
                                
                <div>
                    <h2 class="mb-2 h6 font-weight-bold">
                    <a class="text-dark" href="/spark/aula02_dataframe1.html">Curso de Spark e Python: Aula 02 - DataFrame I</a>
                    </h2>
                    <small class="d-block text-muted">
                        Em  <span class="catlist">
                        
                        <a class="text-capitalize text-muted smoothscroll" href="/categories.html#curso_spark">curso_spark</a><span class="sep">, </span>
                        
                        </span>                   
                    </small>
                    <small class="text-muted">
                        Jun 23, 2019
                    </small>
                </div>
            </div>
        
    </div>
    
</div>
    
<!-- Sticky - add sticky tag to the post you want to highlight here - tags: [sticky] -->
 

 

 

 

 

 




    


 <!--endif page url is / -->
    


<!-- Now the rest of the posts with the usual loop but with an offset:4 on the first page so we can skeep the first 4 posts displayed above -->
    
<div class="row mt-3">
   
    <div class="col-md-8 main-loop">
        
        <h4 class="font-weight-bold spanborder"><span>All Stories</span></h4>
        

        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/rf/introducao.html">Aprendizado por Reforço - Introdução </a>
	</h2>
	<p class="excerpt">
	   Introdução a Aprendizagem por Reforço
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#reinforcement_learning">reinforcement_learning</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Jul 24, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/rf/introducao.html">
	<img class="w-100" src="https://img.ibxk.com.br/2018/10/15/corpos-15114949494090.gif" alt="Aprendizado por Reforço - Introdução ">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/spark/aula04_basic_operations.html">Curso de Spark e Python: Aula 04 - Operações Básicas</a>
	</h2>
	<p class="excerpt">
	   Curso de Spark e PythonAula 04 - Operações Bàsicas
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#curso_spark">curso_spark</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Jun 26, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/spark/aula04_basic_operations.html">
	<img class="w-100" src="https://penseemti.com.br/wp-content/uploads/2017/02/A-importa%CC%82ncia-do-Banco-de-Dados.jpg" alt="Curso de Spark e Python: Aula 04 - Operações Básicas">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/spark/aula03_dataframe2.html">Curso de Spark e Python: Aula 03 - Hello World</a>
	</h2>
	<p class="excerpt">
	   Curso de Spark e PythonAula 03 - DataFrame II
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#curso_spark">curso_spark</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Jun 26, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/spark/aula03_dataframe2.html">
	<img class="w-100" src="https://blog.wavy.global/wp-content/uploads/2019/07/shahadat-shemul-BfrQnKBulYQ-unsplash-780x450.jpg" alt="Curso de Spark e Python: Aula 03 - Hello World">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/spark/aula02_dataframe1.html">Curso de Spark e Python: Aula 02 - DataFrame I</a>
	</h2>
	<p class="excerpt">
	   Curso de Spark e PythonAula 02 - DataFrame I  Para essa aula, você já deverá ter um ambiente spark instalado e configurado. 
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#curso_spark">curso_spark</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Jun 23, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/spark/aula02_dataframe1.html">
	<img class="w-100" src="https://files.keepingcurrentmatters.com/wp-content/uploads/2020/05/05155523/20200506-KCM-Shar.jpg" alt="Curso de Spark e Python: Aula 02 - DataFrame I">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/spark/aula01_introducao.html">Curso de Spark e Python: Aula 01 - Introdução</a>
	</h2>
	<p class="excerpt">
	   Curso de Spark e PythonAula 01 - IntroduçãoOlá, seja bem vindo ao Curso de Spark e Python do Adriano Avelar. Caso encontre algum erro ou deseja entrar em contato, mande um e-mail para...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#curso_spark">curso_spark</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Jun 23, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/spark/aula01_introducao.html">
	<img class="w-100" src="https://miro.medium.com/proxy/0*o3JKcXgKhRaTl3hY.png" alt="Curso de Spark e Python: Aula 01 - Introdução">
	</a>
	</div>

</div>
        
        
        
            <div class="mb-5 d-flex justify-content-between main-loop-card">
<div class="pr-3">
	<h2 class="mb-1 h4 font-weight-bold">
	<a class="text-dark" href="/myfirst-post/">Primeiro Post</a>
	</h2>
	<p class="excerpt">
	   Olá. Me chamo Adriano Avelar, sou Engenheiro da Computação formado pela Universidade Federal do Pará. Tenho Doutorado em Computação pela Universidade Federal de Pernambuco. Já morei e...
	</p>
	<small class="d-block text-muted">
		In <span class="catlist">
		
		<a class="text-capitalize text-muted smoothscroll" href="/categories.html#portfolio">portfolio</a><span class="sep">, </span>
		
		</span>
	</small>
	<small class="text-muted">
		Jan 18, 2019
	</small>
</div>

	<div class="col-md-3 pr-0 text-right">
	<a href="/myfirst-post/">
	<img class="w-100" src="/assets/images/demo1.jpg" alt="Primeiro Post">
	</a>
	</div>

</div>
        
        
        
        <div class="mt-5">
         <!-- Pagination links -->
                  
        </div>
        
    </div>
    
    <div class="col-md-4">
        <div class="sticky-top sticky-top-offset">
    <h4 class="font-weight-bold spanborder"><span>Featured</span></h4>  
    <ol class="list-featured">				
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/rf/introducao.html" class="text-dark">Aprendizado por Reforço - Introdução </a>
                </h6>
                <span class="d-block text-muted">
                    Em <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#reinforcement_learning">reinforcement_learning</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/spark/aula04_basic_operations.html" class="text-dark">Curso de Spark e Python: Aula 04 - Operações Básicas</a>
                </h6>
                <span class="d-block text-muted">
                    Em <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#curso_spark">curso_spark</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/spark/aula03_dataframe2.html" class="text-dark">Curso de Spark e Python: Aula 03 - Hello World</a>
                </h6>
                <span class="d-block text-muted">
                    Em <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#curso_spark">curso_spark</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/spark/aula02_dataframe1.html" class="text-dark">Curso de Spark e Python: Aula 02 - DataFrame I</a>
                </h6>
                <span class="d-block text-muted">
                    Em <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#curso_spark">curso_spark</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/spark/aula01_introducao.html" class="text-dark">Curso de Spark e Python: Aula 01 - Introdução</a>
                </h6>
                <span class="d-block text-muted">
                    Em <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#curso_spark">curso_spark</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
                        
            <li class="mb-4">
            <span>
                <h6 class="font-weight-bold">
                    <a href="/myfirst-post/" class="text-dark">Primeiro Post</a>
                </h6>
                <span class="d-block text-muted">
                    Em <span class="catlist">
                    
                    <a class="text-capitalize text-muted smoothscroll" href="/categories.html#portfolio">portfolio</a><span class="sep">, </span>
                    
                    </span>
                </span>
            </span>
            </li>                
           
    </ol>
</div>     
    </div>
    
</div>



</div>
    </main>


    <!-- Scripts: popper, bootstrap, theme, lunr -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>

    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>

    <script src="/assets/js/theme.js"></script>


    <!-- Footer -->
    <footer class="bg-white border-top p-3 text-muted small">
        <div class="container">
        <div class="row align-items-center justify-content-between">
            <div>
                <span class="navbar-brand mr-2 mb-0"><strong>DataBlow</strong></span>
                <span>Copyright © <script>document.write(new Date().getFullYear())</script>.</span>

                <!--  Github Repo Star Btn-->
                <a class="text-dark ml-1" target="_blank" href="https://github.com/EdsonAvelar"><i class="fab fa-github"></i> Github</a>

            </div>
            <div>
                Feito com <i class="fa fa-heart text-danger" aria-hidden="true"></i> por <a href="/portfolio-pt.html">Adriano Avelar</a>.
            </div>
        </div>
        </div>
    </footer>

    <!-- All this area goes before </body> closing tag --> 


</body>

</html>
